{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Была взята статья Доксы о фильмах. Трудные для машинной разметки места: много неологизмов, составных слов, названий и имен.\n",
    "\n",
    "Выбранный тегсет: https://universaldependencies.org/u/pos/index.html - есть подробное описание, довольно сжатый, поэтому с ним легче сравнивать более подробные теги. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text.txt', 'r', encoding='utf-8') as text_file:\n",
    "    text = text_file.read()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Фильм родился благодаря большому проекту 100 лет настоящего который проходил в Доме мировых культур в Берлине с 2015 по 2019 год Он состоял из многочисленных творческих и образовательных мероприятий и значительную роль в нем играли лекции самых разных гуманитарных исследователей Работа Михаэля Буша его личная интерпретация этого проекта в формате экспериментального видеоколлажа Главная идея которой руководствовался режиссер при его создании связана с Тезисами о философии истории Вальтера Беньямина и в частности с его специфическим понимании времени Фильм задал тон структуре всей программы и подарил ей своей название Ойкономия изучает природу современного финансового капитализма и пытается описать столпы на которых он держится например идею экономического роста Зрителю предлагается задуматься в какой момент и почему наша экономика может существовать только в режиме роста а также какую роль в ней играет долг Фильм позволяет пристально рассмотреть ключевой элемент базиса нашего общеcтвенного устройства и выявить его порочную логику Фильм Марка Баудера широко говорит о проблемах глобального мира но острее всего в нем отражаются вопросы связанные с отношением человека и окружающей среды В кадре появляются ученые философы и исследователи самого разного профиля которых объединяет одно будущее нашей планеты и обеспокоенность стремительно ухудшающейся экологической обстановкой Фильм-эссе выполненный в традициях Харуна Фароки и исследующий природу видения на предмете технологических систем слежения Режиссер Тео Энтони прослеживает историю техник наблюдения и контроля от френологии до современных дронов демонстрируя развитие визуальной культуры и влияние технологий на природу человека Фильм описывает мир тотального контроля рисуя достаточно тревожный антиутопический горизонт но при этом сама картина очень захватывающая зрелищная и актуальная для каждого из нас Эта работа Тима Лейендеккера немного выбивается по своей форме из всей программы Фильм в меньшей степени можно назвать документальным потому что он представляет собой коллаж состоящий из семи разнородных эпизодов Все они выполнены в разных техниках и с нескольких сторон рассматривают резонансный кейс злоумышленного заражения вирусом ВИЧ на секс-вечеринках в городе Гронингене в Нидерландах Режиссер исследует как законодательные и общественные установки определяют наши желания формируют понятия любви вины и морали и какую роль во всех этих социальных процессах играют такой нечеловеческий агент как вирус В эпоху эпидемии эти вопросы кажутся мне невероятно актуальными'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = re.sub('[^\\w\\d\\s\\-]', '', text)\n",
    "text = re.sub(' +', ' ', text)\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pos(word):\n",
    "    mstm = get_mystem_pos(word)\n",
    "    slvnt = get_slovnet_pos(word)\n",
    "    pmrph = get_pymorphy_pos(word)\n",
    "    return [mstm, slvnt, pmrph]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "def get_mystem_pos(word):\n",
    "    m = Mystem()\n",
    "    mystem_pos = []\n",
    "    mystem_word = []\n",
    "    words = m.analyze(word)\n",
    "    ans = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            pos = re.findall('[A-Z]+', word['analysis'][0]['gr'])[0]\n",
    "            ans.append(pos)\n",
    "        except:\n",
    "            if len(re.findall('\\d+', word['text'])) > 0:\n",
    "                ans.append('None')\n",
    "            pass\n",
    "    if len(ans) == 1:\n",
    "        return str(ans).rstrip(\"']\").lstrip(\"['\")\n",
    "    else:\n",
    "        return re.sub(\"'\",'', str(ans).rstrip(']').lstrip('['))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from razdel import sentenize, tokenize\n",
    "from navec import Navec\n",
    "from slovnet import Morph\n",
    "\n",
    "def get_slovnet_pos(word):\n",
    "    chunk = []\n",
    "    for sent in sentenize(word):\n",
    "        tokens = [_.text for _ in tokenize(sent.text)]\n",
    "        chunk.append(tokens)\n",
    "\n",
    "    navec = Navec.load('navec_news_v1_1B_250K_300d_100q.tar')\n",
    "    morph = Morph.load('slovnet_morph_news_v1.tar', batch_size=4)\n",
    "    morph.navec(navec)\n",
    "\n",
    "    markup = next(morph.map(chunk))\n",
    "    ans = []\n",
    "    for token in markup.tokens:\n",
    "        ans.append(token.pos)\n",
    "    if len(ans) == 1:\n",
    "        return str(ans).rstrip(\"']\").lstrip(\"['\")\n",
    "    else:\n",
    "        return re.sub(\"'\",'', str(ans).rstrip(']').lstrip('['))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "\n",
    "def get_pymorphy_pos(word):\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    tokenized_text = word.split(' ')\n",
    "    ans = []\n",
    "    for word in tokenized_text:\n",
    "        ans.append(morph.parse(word)[0].tag.POS)\n",
    "    if len(ans) == 1:\n",
    "        return str(ans).rstrip(\"']\").lstrip(\"['\")\n",
    "    else:\n",
    "        return re.sub(\"'\",'', str(ans).rstrip(']').lstrip('['))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9efe96008cce4d6d8f2ee1559d7c56a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=345.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_words = text.split(' ')\n",
    "mystem_poses = []\n",
    "slovnet_poses = []\n",
    "pymorphy_poses = []\n",
    "for word in tqdm(range(len(text_words))):\n",
    "    get_all = check_pos(text_words[word])\n",
    "    mystem_poses.append(get_all[0])\n",
    "    slovnet_poses.append(get_all[1])\n",
    "    pymorphy_poses.append(get_all[2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corp.txt', 'r', encoding='utf-8') as my_corp:\n",
    "    corp = my_corp.read().split('\\n')\n",
    "    my_poses = []\n",
    "    for word in corp:\n",
    "        pos = word.split('\\t')[1]\n",
    "        my_poses.append(pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>my_pos</th>\n",
       "      <th>slovnet_pos</th>\n",
       "      <th>mystem_pos</th>\n",
       "      <th>pymorphy_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Фильм</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>S</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>родился</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td>V</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>благодаря</td>\n",
       "      <td>ADP</td>\n",
       "      <td>ADP</td>\n",
       "      <td>PR</td>\n",
       "      <td>PREP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>большому</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>A</td>\n",
       "      <td>ADJF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>проекту</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>S</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>вопросы</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>S</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>кажутся</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>V</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>мне</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRON</td>\n",
       "      <td>SPRO</td>\n",
       "      <td>NPRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>невероятно</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADVB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>актуальными</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>A</td>\n",
       "      <td>ADJF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>345 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word my_pos slovnet_pos mystem_pos pymorphy_pos\n",
       "0          Фильм   NOUN        NOUN          S         NOUN\n",
       "1        родился   VERB        VERB          V         VERB\n",
       "2      благодаря    ADP         ADP         PR         PREP\n",
       "3       большому    ADJ         ADJ          A         ADJF\n",
       "4        проекту   NOUN        NOUN          S         NOUN\n",
       "..           ...    ...         ...        ...          ...\n",
       "340      вопросы   NOUN        NOUN          S         NOUN\n",
       "341      кажутся   VERB         ADJ          V         VERB\n",
       "342          мне   PRON        PRON       SPRO         NPRO\n",
       "343   невероятно    ADV         ADV        ADV         ADVB\n",
       "344  актуальными    ADJ         ADJ          A         ADJF\n",
       "\n",
       "[345 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(list(zip(text_words, my_poses, slovnet_poses, mystem_poses, pymorphy_poses)), columns =['word', 'my_pos', 'slovnet_pos', 'mystem_pos', 'pymorphy_pos'])\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Считаем accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pymorphy_acc = {\n",
    "    'ADJ': ['ADJF', 'ADJS', 'COMP'],\n",
    "    'ADP': ['PREP'],\n",
    "    'ADV': ['ADVB', 'PRED'],\n",
    "    'CCONJ': ['CONJ'],\n",
    "    'DET': ['NOUN'],\n",
    "    'NOUN': ['NOUN'],\n",
    "    'NUM': ['NUMR'],\n",
    "    'PART': ['PRCL'],\n",
    "    'PRON': ['NPRO'],\n",
    "    'PROPN': ['NOUN'],\n",
    "    'SCONJ': ['CONJ'],\n",
    "    'VERB': ['VERB',  'INFN', 'PRTF', 'PRTS', 'GRND']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem_acc = {\n",
    "    'ADJ': ['A', 'APRO'],\n",
    "    'ADP': ['PR'],\n",
    "    'ADV': ['ADV', 'ADVPRO'],\n",
    "    'CCONJ': ['CONJ'],\n",
    "    'DET': ['APRO', 'ADVPRO'],\n",
    "    'NOUN': ['S', 'S, S'],\n",
    "    'NUM': ['NUM'],\n",
    "    'PART': ['PART'],\n",
    "    'PRON': ['SPRO'],\n",
    "    'PROPN': ['S'],\n",
    "    'SCONJ': ['CONJ'],\n",
    "    'VERB': ['V']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(acc, machine_tags, my_tags):\n",
    "    acc_counter = 0\n",
    "    for i in range(len(machine_tags)):\n",
    "        if machine_tags[i] in acc[my_tags[i]]:\n",
    "            acc_counter += 1\n",
    "    return acc_counter/len(machine_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for i in range(len(slovnet_poses)):\n",
    "    if slovnet_poses[i] == my_poses[i]:\n",
    "        counter += 1\n",
    "        \n",
    "slovnet_accuracy = counter/len(slovnet_poses)\n",
    "mystem_accuracy = check_accuracy(mystem_acc, mystem_poses, my_poses)\n",
    "pymorphy_accuracy = check_accuracy(pymorphy_acc, pymorphy_poses, my_poses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9130434782608695"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slovnet_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.855072463768116"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystem_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.881159420289855"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pymorphy_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Находим три типа n-грамм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slovnet_ngram(text, ngram_type):\n",
    "    chunk = []\n",
    "    for sent in sentenize(text):\n",
    "        tokens = [_.text for _ in tokenize(sent.text)]\n",
    "        chunk.append(tokens)\n",
    "\n",
    "    navec = Navec.load('navec_news_v1_1B_250K_300d_100q.tar')\n",
    "    morph = Morph.load('slovnet_morph_news_v1.tar', batch_size=4)\n",
    "    morph.navec(navec)\n",
    "    \n",
    "    markup = next(morph.map(chunk))\n",
    "    ngram_tags = ''\n",
    "    for token in markup.tokens:\n",
    "        ngram_tags += token.pos\n",
    "        ngram_tags += ','\n",
    "    if ngram_tags.rstrip(',') == ngram_type:\n",
    "        return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN,VERB ['Фильм родился', 'создании связана', 'Фильм задал', 'Зрителю предлагается', 'экономика может', 'Фильм позволяет', 'вопросы связанные', 'кадре появляются', 'дронов демонстрируя', 'Фильм описывает', 'контроля рисуя', 'коллаж состоящий', 'сторон рассматривают', 'Режиссер исследует', 'установки определяют', 'желания формируют', 'процессах играют']\n",
      "ADJ,NOUN ['большому проекту', 'мировых культур', '2019 год', 'образовательных мероприятий', 'значительную роль', 'гуманитарных исследователей', 'личная интерпретация', 'экспериментального видеоколлажа', 'Главная идея', 'специфическим понимании', 'финансового капитализма', 'экономического роста', 'ключевой элемент', 'порочную логику', 'глобального мира', 'разного профиля', 'экологической обстановкой', 'технологических систем', 'современных дронов', 'визуальной культуры', 'тотального контроля', 'тревожный антиутопический', 'антиутопический горизонт', 'сама картина', 'меньшей степени', 'разнородных эпизодов', 'разных техниках', 'резонансный кейс', 'злоумышленного заражения', 'общественные установки', 'социальных процессах', 'нечеловеческий агент']\n",
      "ADV,VERB ['пристально рассмотреть', 'широко говорит', 'стремительно ухудшающейся', 'немного выбивается', 'можно назвать']\n"
     ]
    }
   ],
   "source": [
    "ngram_types = ['NOUN,VERB', 'ADJ,NOUN', 'ADV,VERB']\n",
    "for ngram_type in ngram_types:\n",
    "    ngram_parts = ngram_type.split(',')\n",
    "    n = len(ngram_parts)\n",
    "    grams = [text_words[i: i + n] for i in range(len(text_words)- n + 1)]\n",
    "    ans = []\n",
    "    for gram in grams:\n",
    "        if get_slovnet_ngram(' '.join(gram), ngram_type):\n",
    "            ans.append(' '.join(gram))\n",
    "    print(ngram_type, ans)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
